{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=====[ Setup - don't modify ]=====\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "from plotting_tools import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "purpose_of_run = 'for_plotting'\n",
    "#purpose_of_run = 'for_tables'\n",
    "\n",
    "desired_specificity_bin_width = 0.01 if purpose_of_run=='for_tables' else 0.05\n",
    "\n",
    "training_only_scenarios_dict = collections.OrderedDict([\n",
    "    ('default', {'age_split': False, 'full_desc': 'full default, no encoding or filtering, one age bin, default features selection', 'latex_desc': 'all default'}),\n",
    "    ('balanced_tally', {'age_split': False, 'full_desc': 'no encoding or filtering, one age bin, tally features selection, balanced', 'latex_desc': 'tally selection, others default, balanced'}),\n",
    "    ('age_binned', {'age_split': True, 'young_prefix': 'young_age', 'old_prefix': 'old_age', 'full_desc': 'no encoding or filtering, tally features selection, age binned', 'latex_desc': 'age binned, no encoding'}),\n",
    "    ('age_binned_encoded', {'age_split': True, 'young_prefix': 'young_encoded', 'old_prefix': 'old_encoded', 'full_desc': 'tally features selection, age binned, encoded', 'latex_desc': 'age binned, with encoding'}),\n",
    "])\n",
    "\n",
    "clinical_scenarios_dict = collections.OrderedDict([\n",
    "     ('scalar_encoding_all_features', {'encoding': 'scalar', 'feature_restriction': None, 'latex_desc': 'Scalar, all features'}),\n",
    "     ('scalar_encoding_restricted_features', {'encoding': 'scalar', 'feature_restriction': [], 'latex_desc': 'Scalar, restricted features'}),\n",
    "     ('production_encoding_restricted_features', {'encoding': 'production', 'feature_restriction': [], 'latex_desc': 'Encoded, restricted features'}),\n",
    "     ('production_encoding_restricted_features_proportional_loss', {'encoding': 'production', 'feature_restriction': [], 'inject_loss': 'proportional', 'latex_desc': 'Proportional loss'}),\n",
    "     ('exact_production_optimization', {'encoding': 'production', 'feature_restriction': [], 'inject_loss': 'proportional', 'latex_desc': 'Exact prod'}),\n",
    "     ('age_binned_official', {'age_split': True, 'young_prefix': 'young_age_official', 'old_prefix': 'old_age_official', 'full_desc': 'no encoding or filtering, official features selection, age binned', 'latex_desc': 'Age silo variant', 'color': 'red', 'linestyle': 'dashed', 'linewidth': 5}),\n",
    "     ('age_binned_encoded_official', {'age_split': True, 'young_prefix': 'young_encoded_official', 'old_prefix': 'old_encoded_official','full_desc': 'official features selection, encoded', 'latex_desc': 'Severity-level feature encoding variant', 'color': 'blue', 'linestyle': 'dotted', 'linewidth': 5}),\n",
    "     ('engineering_official', {'age_split': True, 'young_prefix': 'young_engineered_official', 'old_prefix': 'old_engineered_official','full_desc': 'official features selection, encoded, feature engineering', 'latex_desc': 'Aggregate features variant'}), \n",
    "     ('exact_optimization', {'age_split': True, 'young_prefix': 'young_age_exact', 'old_prefix': 'old_age_exact', 'latex_desc': 'Exact'}),\n",
    "])\n",
    "algorithms = collections.OrderedDict([    \n",
    "    ('guardian.qnnaire.3-', {'filter_key': 'Guardian Qnnaire Version', 'filter_value': '3-'}),\n",
    "    ('guardian.qnnaire.4+', {'filter_key': 'Guardian Qnnaire Version', 'filter_value': '4+'}),\n",
    "    ('video.module1', {'filter_key': ' Video Version', 'filter_value': 'module1'}),\n",
    "    ('video.module2', {'filter_key': ' Video Version', 'filter_value': 'module2'})\n",
    "])\n",
    "clinical_scenarios = clinical_scenarios_dict.keys()\n",
    "\n",
    "metrics_to_do = collections.OrderedDict([\n",
    "    ('AUC', {'desc': 'AUC'}),\n",
    "    ('average_recall', {'desc': 'Average Recall'}),\n",
    "    ('average_precision_dataset', {'desc': 'Average Precision [Dataset]', 'key': 'average precision [Dataset]'}),\n",
    "    ('average_precision_dataset', {'desc': 'Average Precision [Dataset]'}),\n",
    "    ('autism_recall', {'desc': 'Autism Recall'}),\n",
    "    ('not_recall', {'desc': 'Not Recall'})\n",
    "])\n",
    "ages_dict = {\n",
    "    'guardian.qnnaire.3-': 'young',\n",
    "    'guardian.qnnaire.4+': 'old',\n",
    "    'questionnaire': 'combined',\n",
    "    'video.module.1': 'young',\n",
    "    'video.module.2': 'old',\n",
    "    'video': 'combined'\n",
    "}\n",
    "print 'baseline stuff defined. purpose of run is ', purpose_of_run\n",
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_recent_filename_with_prefix_suffix_and_date(prefix, suffix, directory='.', debug=False, max_date=None):\n",
    "    ''' Get most recent filename with a given prefix, suffix, and date in a given directory '''\n",
    "    filenames_matching_constraints = [my_file for my_file in os.listdir(directory) if my_file.startswith(prefix)\\\n",
    "                                      and my_file.endswith(suffix)]\n",
    "    all_dates = []\n",
    "    max_date_so_far = None\n",
    "    latest_filename = None\n",
    "    if debug:\n",
    "        print 'get most recent file from ', filenames_matching_constraints\n",
    "    for filename in filenames_matching_constraints:\n",
    "        date_str = filename.split(prefix)[1].split(suffix)[0]\n",
    "        if date_str[0]=='_': date_str = date_str[1:]\n",
    "        if debug:\n",
    "            print 'date_str: ', date_str\n",
    "        try:\n",
    "            #print 'get my_datetime from date_str: ', date_str\n",
    "            my_datetime = datetime.datetime.strptime(date_str, '%m.%d.%y')\n",
    "            if max_date is not None and my_datetime > max_date:\n",
    "                print 'datetime ', my_datetime, ' is past max date of ', max_date, ', so skip this.'\n",
    "                continue\n",
    "        except:\n",
    "            'File ', filename, ', has no recognized datetime'\n",
    "            continue\n",
    "        if debug:\n",
    "            print 'my_datetime: ', my_datetime\n",
    "        if max_date_so_far is None:\n",
    "            max_date_so_far = my_datetime\n",
    "            latest_filename = directory + '/' + filename\n",
    "        elif my_datetime > max_date_so_far:\n",
    "            max_date_so_far = my_datetime\n",
    "            latest_filename = directory + '/' + filename\n",
    "    if debug:\n",
    "        print 'most recent one is ', latest_filename\n",
    "    #print 'directory: ', directory, ', prefix: ', prefix, ', suffix: ', suffix\n",
    "    return latest_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load baseline comparison models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataFilePath = # PATH TO MCHAT-R BASELINE DATA FILE\n",
    "f = open(dataFilePath, 'rU')\n",
    "dictReader = csv.DictReader(f, delimiter='\\t')\n",
    "data = []\n",
    "for row in dictReader:\n",
    "    data.append(row)\n",
    "\n",
    "mchat_df = pd.DataFrame(data)\n",
    "mchat_df['Mchat Final Score'] = mchat_df['Mchat Final Score'].apply(lambda x: None if x=='' else float(x))\n",
    "mchat_df['Patient Id'] = mchat_df['Patient Id'].astype(int)\n",
    "mchat_df.shape\n",
    "\n",
    "dataFilePath = # PATH DO CBCL BASELINE DATA FILE\n",
    "f = open(dataFilePath, 'rU')\n",
    "dictReader = csv.DictReader(f, delimiter='\\t')\n",
    "data = []\n",
    "for row in dictReader:\n",
    "    data.append(row)\n",
    "\n",
    "cbcl_df = pd.DataFrame(data)\n",
    "\n",
    "cbcl_df['CBCL Score'] = cbcl_df['Autism Spectrum Problems Total Score'].apply(lambda x: float(x) if len(x)>0 else np.NaN)\n",
    "cbcl_df['Patient Id'] = cbcl_df['Patient Id'].astype(int)\n",
    "cbcl_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_clinical_latex_tables(clinical_metric_dfs, desc):\n",
    "    metrics_to_combine = ['AUC', 'not_precision', 'autism_precision', 'average_precision_dataset', 'not_recall']\n",
    "    comb_latex_df = None\n",
    "    full_metric_dfs = cp.deepcopy(clinical_metric_dfs)\n",
    "\n",
    "    def get_metric_column(metric_name):\n",
    "        if 'key' in metrics_to_do[metric_name].keys():\n",
    "            return metrics_to_do[metric_name]['key']\n",
    "        else:\n",
    "            return metric_name.replace('_', ' ')\n",
    "\n",
    "    ### Determine training results and merge with clinical results df\n",
    "    for metric_name, metric_details in metrics_to_do.iteritems():\n",
    "        for algorithm in algorithms.keys():\n",
    "            if algorithm == 'video.module.1' or algorithm == 'video.module.2':\n",
    "                continue\n",
    "\n",
    "\n",
    "            optimized_filename = '../training_code/optimized_settings/optimal_parameters_for_'+algorithm+'_only_official_features.csv'\n",
    "            optimized_df = pd.read_csv(optimized_filename)\n",
    "            print 'for algorithm: ', algorithm, ', filename: ', optimized_filename\n",
    "            print 'clinical scenarios dict: ', clinical_scenarios_dict\n",
    "            print 'scenarios: ', optimized_df['scenario'].values\n",
    "            optimized_df['latex_scenario'] = [clinical_scenarios_dict[scenario]['latex_desc'] for scenario in\\\n",
    "                                    optimized_df['scenario'].values ]\n",
    "            if 'average precision [Dataset]' not in optimized_df.columns:\n",
    "                optimized_df['average precision [Dataset]'] = 0.5*(optimized_df['autism precision [Dataset]'] + optimized_df['not precision [Dataset]'])\n",
    "            print 'metric: ', metric_name, ', algo: ', algorithm, ', optimized_df: ', optimized_df\n",
    "\n",
    "\n",
    "\n",
    "            columns_to_use = ['latex_scenario', get_metric_column(metric_name)]\n",
    "                    \n",
    "            print 'optimized_df columns: ', list(optimized_df.columns)\n",
    "            print 'columns_to_use: ', columns_to_use\n",
    "            relevant_part_of_df = cp.deepcopy(optimized_df[columns_to_use])\n",
    "            relevant_part_of_df.columns = ['latex_scenario', 'train '+algorithm]\n",
    "            full_metric_dfs[metric_name] = full_metric_dfs[metric_name].merge(relevant_part_of_df, on='latex_scenario',\n",
    "                                                            how='outer')\n",
    "\n",
    "        print 'For metric: ', metric_name, ', cols: ', list(full_metric_dfs[metric_name].columns)\n",
    "        full_metric_dfs[metric_name].index = full_metric_dfs[metric_name]['latex_scenario']\n",
    "        full_metric_dfs[metric_name].index.names = [metric_details['desc']]\n",
    "        #numeric_columns = [ 'guardian.qnnaire.3-', 'guardian.qnnaire.4+', 'video.module.1', 'video.module.2', 'train guardian.qnnaire.3-', 'train guardian.qnnaire.4+', 'train video.module.1', 'train video.module.2']\n",
    "        numeric_columns = [ 'guardian.qnnaire.3-', 'guardian.qnnaire.4+', 'questionnaire']\n",
    "        full_metric_dfs[metric_name][numeric_columns] = full_metric_dfs[metric_name][numeric_columns].round(2)\n",
    "\n",
    "        print 'For metric: ', metric_name, ', after merging training optimizations, full avg recall df: ', full_metric_dfs[metric_name]\n",
    "\n",
    "        cols_to_write = ['guardian.qnnaire.3-', 'guardian.qnnaire.4+', 'video.module.1', 'video.module.2', 'train guardian.qnnaire.3-', 'train guardian.qnnaire.4+', 'train video.module.1', 'train video.module.2']\n",
    "        latex_filename = 'clinical_optimized_performance_'+metric_name+'.tex'\n",
    "\n",
    "        to_latex_df = cp.deepcopy(full_metric_dfs[metric_name][cols_to_write])\n",
    "            #to_latex_df.columns = ['scenario', 'All '+metric, '$< 4$ '+metric, '$\\geq 4$ '+metric]\n",
    "\n",
    "\n",
    "        to_latex_df.columns = ['$< 4$ yr', '$\\\\geq 4$ yr', 'All']\n",
    "        to_latex_df = to_latex_df[['All', '$< 4$ yr', '$\\\\geq 4$ yr']]\n",
    "        to_latex_df.to_latex(buf=latex_filename, index=True)\n",
    "        to_latex_df.columns = ['All '+metric_name, '$< 4$ yr '+metric_name, '$\\\\geq 4$ yr '+metric_name]\n",
    "\n",
    "\n",
    "        if metric_name in metrics_to_combine:\n",
    "            if comb_latex_df is None:\n",
    "                comb_latex_df = cp.deepcopy(to_latex_df)\n",
    "            else:\n",
    "                comb_latex_df = comb_latex_df.merge(to_latex_df, left_index=True, right_index=True)\n",
    "\n",
    "    comb_latex_df.index.name = 'scenario'\n",
    "    print 'comb_latex_df to write, before conversions of strings: ', comb_latex_df\n",
    "    latex_string = comb_latex_df.to_latex(escape=False, index=True)\n",
    "    #### These lines are to make the table multicell and put the metrics in the multicell contents\n",
    "    latex_string = latex_string.replace('lrrrrrrrrr', 'p{3.5cm}|p{1cm}p{1cm}p{1cm}|p{1cm}p{1cm}p{1cm}|p{1cm}p{1cm}p{1cm}')\n",
    "    latex_string = latex_string.replace('\\\\toprule', '\\\\hline\\n & \\\\multicolumn{3}{|c|}{AUC} & \\\\multicolumn{3}{c|}{Average precision @ 80\\% sensitivity} & \\\\multicolumn{3}{c}{Specificity @ 80\\% sensitivity} \\\\\\\\')\n",
    "    #### This is to correct a multicell display error\n",
    "    latex_string = latex_string.replace('\\\\midrule', '\\\\hline')\n",
    "    latex_string = latex_string.replace('\\\\bottomrule', '\\\\hline')\n",
    "    ##### This part is to remove the extra column labels that will now go in the header of the table\n",
    "    #latex_string = latex_string.replace(' AUC ', '')\n",
    "    #latex_string = latex_string.replace(' autism_recall ', '')\n",
    "    #latex_string = latex_string.replace(' not_recall ', '')\n",
    "    #latex_string = latex_string.replace(' average_precision_dataset ', '')\n",
    "    ##### This part is to deal with some weird bug that splits a row\n",
    "    latex_string = latex_string.replace('scenario                 &          &            &               &                    &                      &                         &                 &                   &                      \\\\\\\\', '')\n",
    "    latex_string = latex_string.replace('scenario                 &          &            &               &                                 &                                   &                                      &                 &                   &                      \\\\\\\\', '')\n",
    "    latex_string = latex_string.replace('{}', 'scenario')\n",
    "\n",
    "\n",
    "    latex_file_out = open(desc+'_clinical_optimization_results_questionnaire_combined.tex', 'w')\n",
    "    latex_file_out.write(latex_string)\n",
    "    latex_file_out.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## def make_clinical_latex_tables(clinical_metric_dfs, desc):\n",
    "    metrics_to_combine = ['AUC', 'average_precision_dataset', 'not_recall']\n",
    "    comb_latex_df = None\n",
    "    full_metric_dfs = cp.deepcopy(clinical_metric_dfs)\n",
    "\n",
    "    def get_metric_column(metric_name):\n",
    "        if 'key' in metrics_to_do[metric_name].keys():\n",
    "            return metrics_to_do[metric_name]['key']\n",
    "        else:\n",
    "            return metric_name.replace('_', ' ')\n",
    "\n",
    "    ### Determine training results and merge with clinical results df\n",
    "    for metric_name, metric_details in metrics_to_do.iteritems():\n",
    "        for algorithm in algorithms.keys():\n",
    "            if algorithm == 'video.module.1' or algorithm == 'video.module.2':\n",
    "                continue\n",
    "\n",
    "\n",
    "            optimized_filename = '../training_code/optimized_settings/optimal_parameters_for_'+algorithm+'_only_official_features.csv'\n",
    "            optimized_df = pd.read_csv(optimized_filename)\n",
    "            print 'for algorithm: ', algorithm, ', filename: ', optimized_filename\n",
    "            print 'clinical scenarios dict: ', clinical_scenarios_dict\n",
    "            print 'scenarios: ', optimized_df['scenario'].values\n",
    "            optimized_df['latex_scenario'] = [clinical_scenarios_dict[scenario]['latex_desc'] for scenario in\\\n",
    "                                    optimized_df['scenario'].values ]\n",
    "            print 'metric: ', metric_name, ', algo: ', algorithm, ', optimized_df: ', optimized_df\n",
    "\n",
    "\n",
    "\n",
    "            columns_to_use = ['latex_scenario', get_metric_column(metric_name)]\n",
    "            relevant_part_of_df = cp.deepcopy(optimized_df[columns_to_use])\n",
    "            relevant_part_of_df.columns = ['latex_scenario', 'train '+algorithm]\n",
    "            full_metric_dfs[metric_name] = full_metric_dfs[metric_name].merge(relevant_part_of_df, on='latex_scenario',\n",
    "                                                            how='outer')\n",
    "\n",
    "        print 'For metric: ', metric_name, ', cols: ', list(full_metric_dfs[metric_name].columns)\n",
    "        full_metric_dfs[metric_name].index = full_metric_dfs[metric_name]['latex_scenario']\n",
    "        full_metric_dfs[metric_name].index.names = [metric_details['desc']]\n",
    "        #numeric_columns = [ 'guardian.qnnaire.3-', 'guardian.qnnaire.4+', 'video.module.1', 'video.module.2', 'train guardian.qnnaire.3-', 'train guardian.qnnaire.4+', 'train video.module.1', 'train video.module.1']\n",
    "        numeric_columns = [ 'guardian.qnnaire.3-', 'guardian.qnnaire.4+', 'questionnaire']\n",
    "        full_metric_dfs[metric_name][numeric_columns] = full_metric_dfs[metric_name][numeric_columns].round(3)\n",
    "\n",
    "        print 'For metric: ', metric_name, ', after merging training optimizations, full avg recall df: ', full_metric_dfs[metric_name]\n",
    "\n",
    "        cols_to_write = ['guardian.qnnaire.3-', 'guardian.qnnaire.4+', 'video.module.1', 'video.module.2', 'train guardian.qnnaire.3-', 'train guardian.qnnaire.4+', 'train video.module.1', 'train video.module.1']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        latex_filename = 'clinical_optimized_performance_'+metric_name+'.tex'\n",
    "\n",
    "        to_latex_df = cp.deepcopy(full_metric_dfs[metric_name][cols_to_write])\n",
    "        to_latex_df.columns = ['scenario', 'All '+metric, '$< 4$ '+metric, '$\\geq 4$ '+metric]\n",
    "\n",
    "\n",
    "        to_latex_df.columns = ['$< 4$', '$\\geq 4$', 'All']\n",
    "        to_latex_df = to_latex_df[['All', '$< 4$', '$\\geq 4$']]\n",
    "        to_latex_df.to_latex(buf=latex_filename, index=True)\n",
    "        to_latex_df.columns = ['All '+metric_name, '$< 4$ '+metric_name, '$\\geq 4$ '+metric_name]\n",
    "\n",
    "\n",
    "        if metric_name in metrics_to_combine:\n",
    "            if comb_latex_df is None:\n",
    "                comb_latex_df = cp.deepcopy(to_latex_df)\n",
    "            else:\n",
    "                comb_latex_df = comb_latex_df.merge(to_latex_df, left_index=True, right_index=True)\n",
    "\n",
    "    comb_latex_df.index.name = 'scenario'\n",
    "    latex_string = comb_latex_df.to_latex(escape=False, index=True)\n",
    "    #### These lines are to make the table multicell and put the metrics in the multicell contents\n",
    "    latex_string = latex_string.replace('lrrrrrrrrr', 'p{3.5cm}|p{1cm}p{1cm}p{1cm}|p{1cm}p{1cm}p{1cm}|p{1cm}p{1cm}p{1cm}')\n",
    "    latex_string = latex_string.replace('\\\\toprule', '\\\\hline\\n & \\\\multicolumn{3}{|c|}{AUC} & \\\\multicolumn{3}{c|}{Average precision} & \\\\multicolumn{3}{c}{Specificity} \\\\\\\\')\n",
    "    #### This is to correct a multicell display error\n",
    "    latex_string = latex_string.replace('\\\\midrule', '\\\\hline')\n",
    "    latex_string = latex_string.replace('\\\\bottomrule', '\\\\hline')\n",
    "    ##### This part is to remove the extra column labels that will now go in the header of the table\n",
    "    latex_string = latex_string.replace(' AUC ', '')\n",
    "    latex_string = latex_string.replace(' autism_recall ', '')\n",
    "    latex_string = latex_string.replace(' not_recall ', '')\n",
    "    latex_string = latex_string.replace(' average_precision_dataset ', '')\n",
    "    ##### This part is to deal with some weird bug that splits a row\n",
    "    latex_string = latex_string.replace('scenario                 &          &            &               &                    &                      &                         &                 &                   &                      \\\\\\\\', '')\n",
    "    latex_string = latex_string.replace('scenario                 &          &            &               &                                 &                                   &                                      &                 &                   &                      \\\\\\\\', '')\n",
    "    latex_string = latex_string.replace('{}', 'scenario')\n",
    "\n",
    "\n",
    "    latex_file_out = open(desc+'_clinical_optimization_results_questionnaire_combined.tex', 'w')\n",
    "    latex_file_out.write(latex_string)\n",
    "    latex_file_out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models and clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_directory = # INSERT PATH TO YOUR OWN MODELS TO VALIDATE\n",
    "clinical_data_directory = # INSERT PATH TO YOUR OWN CLINICAL VALIDATION DATA\n",
    "input_video_data = clinical_data_directory + 'clinical_study_video_model_application_response.csv'\n",
    "input_questionnaire_data = clinical_data_directory + 'clinical_study_guardian_qnnaire_model_application_response.csv'\n",
    "\n",
    "### This is what we will save out\n",
    "clinical_data_dfs = {}   ### one dataframe for each algorithm (qnnaires and videos)\n",
    "models_structure_dict = collections.OrderedDict(    ### a model for each combination of algorithm and scenario\n",
    "    (algorithm, {}) for algorithm in algorithms\n",
    ")\n",
    "\n",
    "def convert_to_int_str(in_val):\n",
    "    try:\n",
    "        return str(int(in_val))\n",
    "    except:\n",
    "        return 'missing'\n",
    "\n",
    "### Get data to use\n",
    "for algorithm, algo_specs in algorithms.iteritems():\n",
    "    data_filename_to_use = input_video_data if 'video' in algorithm else input_questionnaire_data\n",
    "    clinical_data_dfs[algorithm] = pd.read_csv(data_filename_to_use)\n",
    "    filter_column = algo_specs['filter_key']\n",
    "    filter_value = algo_specs['filter_value']\n",
    "    clinical_data_dfs[algorithm] = clinical_data_dfs[algorithm][clinical_data_dfs[algorithm][filter_column]==filter_value]\n",
    "    ### Convert relevant columns to strings:\n",
    "    for column in clinical_data_dfs[algorithm].columns:\n",
    "        if 'video_instrument' in column or 'guardian_qnnaire_instrument' in column:\n",
    "            clinical_data_dfs[algorithm][column] = [convert_to_int_str(ele) for ele in clinical_data_dfs[algorithm][column].values]\n",
    "            #.astype(str\n",
    "            \n",
    "    #### Filter to only IDs that are also in the clinical video dataset\n",
    "    print 'algorithm ', algorithm, ', num children before video ID filter: ', len(clinical_data_dfs[algorithm].index)\n",
    "    clinical_data_dfs[algorithm] = clinical_data_dfs[algorithm][clinical_data_dfs[algorithm]['Patient Id'].isin(video_df_ids)]\n",
    "    print 'algorithm ', algorithm, ', num children after video ID filter: ', len(clinical_data_dfs[algorithm].index)\n",
    "\n",
    "\n",
    "    \n",
    "### Get models\n",
    "for algorithm in algorithms.keys():\n",
    "    for scenario, scenario_details in clinical_scenarios_dict.iteritems():\n",
    "        if ages_dict[algorithm] == 'young':\n",
    "            print 'scenario: ', scenario, ', details: ', scenario_details\n",
    "            prefix = scenario_details['young_prefix']\n",
    "        elif ages_dict[algorithm] == 'old':\n",
    "            prefix = scenario_details['old_prefix']\n",
    "        this_desc  = algorithm + '_' + scenario\n",
    "        print 'Get and apply models for algorithm: ', algorithm, ', scenario: ', scenario\n",
    "        suffix = '.model'\n",
    "        \n",
    "        model_filename = get_most_recent_filename_with_prefix_suffix_and_date(prefix=prefix, suffix=suffix,\n",
    "                                            directory=model_directory, debug=True, max_date=datetime.datetime(2017,1,5))\n",
    "        \n",
    "        if scenario == 'scalar_encoding_all_features':\n",
    "            print 'Cannot apply model to clinical data for scenario: ', scenario\n",
    "            continue\n",
    "        if model_filename is None:\n",
    "            print 'Scenario: ', scenario, ', algorithm: ', algorithm, ', not defined. Skip.'\n",
    "            continue\n",
    "        #print 'load model ', model_filename\n",
    "        models_structure_dict[algorithm][scenario] = load_model(model_filename)\n",
    "        print 'Got model for ', this_desc\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine model responses and recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "                    \n",
    "def get_metric_val(metric_name, AUC, autism_recall, not_recall, autism_precision, not_precision):\n",
    "    \n",
    "    print 'metric_name: ', metric_name\n",
    "    metric_value = None\n",
    "    if metric_name == 'AUC':\n",
    "        metric_value = AUC\n",
    "    elif metric_name == 'average_recall':\n",
    "        metric_value = (autism_recall + not_recall) / 2.\n",
    "    elif metric_name == 'average_precision_dataset':\n",
    "        metric_value = (autism_precision + not_precision) / 2.\n",
    "    elif metric_name == 'autism_recall':\n",
    "        metric_value = autism_recall\n",
    "    elif metric_name == 'not_recall':\n",
    "        metric_value = not_recall\n",
    "    else:\n",
    "        raise NotImplementedError('Metric: '+metric_name+' not implemented yet')   \n",
    "    return metric_value\n",
    "\n",
    "def get_optimized_metrics_from_model_responses(responses_2d, class_names, class_priors, truth_vals, desired_autism_recall, desc, find_optimal_dunno=True):\n",
    "    best_delta_from_desired_so_far = None\n",
    "    best_model_metrics_so_far = None\n",
    "    best_threshold_so_far = None\n",
    "    responses = [ele[0] for ele in responses_2d]\n",
    "    threshold_vals = np.arange(0.3, 0.9, 0.01)\n",
    "    #print 'threshold vals: ', threshold_vals\n",
    "    for threshold in threshold_vals:\n",
    "        print 'threshold: ', threshold\n",
    "        print 'responses: ', responses\n",
    "        y_predicted_without_dunno = np.array(['autism' if response > threshold else 'not' for response in responses])\n",
    "        y_predicted_with_dunno = np.array(y_predicted_without_dunno)\n",
    "        model_metrics = get_classifier_performance_metrics(class_names, class_priors, truth_vals, \n",
    "                    y_predicted_without_dunno, y_predicted_with_dunno, responses_2d)\n",
    "        autism_recall = model_metrics['without_dunno']['dataset_recall_per_class']['autism']\n",
    "        not_recall = model_metrics['without_dunno']['dataset_recall_per_class']['not']\n",
    "        delta_from_desired = abs(autism_recall - desired_autism_recall)\n",
    "        if best_delta_from_desired_so_far is None or best_delta_from_desired_so_far > delta_from_desired:\n",
    "            best_model_metrics_so_far = cp.deepcopy(model_metrics)\n",
    "            best_delta_from_desired_so_far = delta_from_desired\n",
    "            best_threshold_so_far = threshold\n",
    "            print 'best so far'\n",
    "    \n",
    "    print 'For ', desc, ', best threshold was: ', best_threshold_so_far, ', with autism recall: ',\\\n",
    "        best_model_metrics_so_far['without_dunno']['dataset_recall_per_class']['autism'], ', not recall: ',\\\n",
    "        best_model_metrics_so_far['without_dunno']['dataset_recall_per_class']['not']\n",
    "        \n",
    "    if find_optimal_dunno:\n",
    "        def get_outcome_with_dunno(response, dunno_range):\n",
    "            if response < dunno_range[0]: return 'not'\n",
    "            elif response < dunno_range[1]: return 'dunno'\n",
    "            else: return 'autism'\n",
    "            \n",
    "        low_offset_grid = np.arange(0., 0.2, 0.02)\n",
    "        high_offset_grid = np.arange(0., 0.2, 0.02)\n",
    "        coverage_limit = 0.75\n",
    "#         print 'Try dunno loop'\n",
    "        best_average_recall_so_far_with_dunno = None\n",
    "        best_model_metrics_so_far_with_dunno = None\n",
    "        for low_offset in low_offset_grid:\n",
    "            low_threshold = best_threshold_so_far - low_offset\n",
    "            for high_offset in high_offset_grid:\n",
    "                high_threshold = best_threshold_so_far + high_offset\n",
    "                dunno_range = [low_threshold, high_threshold]\n",
    "                y_predicted_without_dunno = np.array(['autism' if response > best_threshold_so_far else 'not' for response in responses])\n",
    "                y_predicted_with_dunno = np.array([get_outcome_with_dunno(response, dunno_range) for response in responses])\n",
    "                #print 'zipy zip: ', zip(responses, y_predicted_without_dunno, y_predicted_with_dunno, )\n",
    "                dunno_model_metrics = get_classifier_performance_metrics(class_names, class_priors, truth_vals,y_predicted_without_dunno, y_predicted_with_dunno, responses_2d)\n",
    "                autism_recall = dunno_model_metrics['without_dunno']['dataset_recall_per_class']['autism']\n",
    "                not_recall = dunno_model_metrics['without_dunno']['dataset_recall_per_class']['not']\n",
    "                coverage = dunno_model_metrics['with_dunno']['dataset_classification_rate']\n",
    "                autism_recall_with_dunno = dunno_model_metrics['with_dunno']['dataset_recall_per_class_where_classified']['autism']\n",
    "                not_recall_with_dunno = dunno_model_metrics['with_dunno']['dataset_recall_per_class_where_classified']['not']\n",
    "#                 print 'For desc ', desc, ', threshold: ', best_threshold_so_far, ', dunno: ', dunno_range, ', coverage: ', coverage\n",
    "                if coverage < coverage_limit: continue\n",
    "                average_recall = 0.5*(autism_recall + not_recall)\n",
    "                average_recall_with_dunno = 0.5*(autism_recall_with_dunno + not_recall_with_dunno)\n",
    "#                 print 'autism_recall with dunno now: ', autism_recall_with_dunno, ', improved from ', autism_recall\n",
    "#                 print 'not_recall with dunno now: ', not_recall_with_dunno, ', improved from ', not_recall\n",
    "\n",
    "\n",
    "                if best_average_recall_so_far_with_dunno is not None and best_average_recall_so_far_with_dunno >= average_recall_with_dunno: continue\n",
    "                \n",
    "                print 'get best with dunno'\n",
    "                best_model_metrics_so_far_with_dunno  = dunno_model_metrics\n",
    "                best_average_recall_so_far_with_dunno = average_recall_with_dunno\n",
    "                print 'New best found'\n",
    "        return best_model_metrics_so_far_with_dunno\n",
    "    else:\n",
    "        return best_model_metrics_so_far\n",
    "\n",
    "\n",
    "def append_metric_results_to_dict_of_lists(clinical_optimized_metric_dict, model_metrics, scenario):\n",
    "    ''' After appending to the results, results will get turned into a dataframe '''     \n",
    "    autism_recall = model_metrics['without_dunno']['dataset_recall_per_class']['autism']\n",
    "    not_recall = model_metrics['without_dunno']['dataset_recall_per_class']['not']\n",
    "    autism_precision = model_metrics['without_dunno']['dataset_precision_per_class']['autism']\n",
    "    not_precision = model_metrics['without_dunno']['dataset_precision_per_class']['not']\n",
    "            \n",
    "    print 'get recalls and preciions with dunno'\n",
    "    autism_recall_with_dunno = model_metrics['with_dunno']['dataset_recall_per_class_where_classified']['autism']\n",
    "    not_recall_with_dunno = model_metrics['with_dunno']['dataset_recall_per_class_where_classified']['not']\n",
    "    autism_precision_with_dunno = model_metrics['with_dunno']['dataset_precision_per_class_where_classified']['autism']\n",
    "    not_precision_with_dunno = model_metrics['with_dunno']['dataset_precision_per_class_where_classified']['not']\n",
    "\n",
    "\n",
    "            \n",
    "    print 'For this_desc: ', this_desc\n",
    "    print 'precisions: ', autism_precision, ', ', not_precision\n",
    "    AUC = model_metrics['without_dunno']['auc']\n",
    "    for metric_name, metric_details in metrics_to_do.iteritems():\n",
    "        metric_value = get_metric_val(metric_name, AUC, autism_recall, not_recall, autism_precision, not_precision)\n",
    "        print 'Metric value for ', this_desc, ', metric: ', metric_name, ': ', metric_value\n",
    "        #print 'clinical_optimized_avg_recall_dict[algorithm]: ', clinical_optimized_avg_recall_dict[algorithm]\n",
    "        clinical_optimized_metric_dict[metric_name][algorithm].append(metric_value)\n",
    "        if scenario != 'engineering_official': continue\n",
    "        print 'get dunno version of metric:'\n",
    "        metric_val_with_dunno = get_metric_val(metric_name, AUC, autism_recall_with_dunno, \n",
    "                            not_recall_with_dunno, autism_precision_with_dunno, not_precision_with_dunno)\n",
    "        clinical_optimized_metric_dict[metric_name][algorithm].append(metric_val_with_dunno)\n",
    "    return clinical_optimized_metric_dict\n",
    "\n",
    "         AUC = model_metrics['without_dunno']['auc']\n",
    "         not_recall = model_metrics['without_dunno']['dataset_recall_per_class']['not']\n",
    "         autism_precision = model_metrics['without_dunno']['reallife_precision_per_class']['autism']\n",
    "         not_precision = model_metrics['without_dunno']['reallife_precision_per_class']['not']\n",
    "\n",
    "\n",
    "\n",
    "class_priors = [(1.0/2.0), (1.0/2.0)]\n",
    "\n",
    "### This is what we will fill in \n",
    "old_clinical_optimized_metric_dict = collections.OrderedDict()\n",
    "for metric_name, metric_details in metrics_to_do.iteritems():\n",
    "    old_clinical_optimized_metric_dict[metric_name] = collections.OrderedDict(\n",
    "        (algorithm, []) for algorithm in algorithms\n",
    "    )\n",
    "\n",
    "for metric_name, metric_details in metrics_to_do.iteritems():\n",
    "    old_clinical_optimized_metric_dict[metric_name]['scenario'] = clinical_scenarios + ['engineering_official_dunno']\n",
    "    old_clinical_optimized_metric_dict[metric_name]['latex_scenario'] = [value['latex_desc'] for value in clinical_scenarios_dict.values()]+['with 25\\\\% inconclusive allowance']\n",
    "\n",
    "# metrics_to_do = collections.OrderedDict([\n",
    "#     ('average_recall', {'desc': 'Average Recall'}),\n",
    "#     ('autism_recall', {'desc': 'Autism Recall'}),\n",
    "#     ('not_recall', {'desc': 'Not Recall'})\n",
    "# ])\n",
    "for algorithm in algorithms.keys():\n",
    "    for scenario in clinical_scenarios:\n",
    "        this_desc  = algorithm + '_' + scenario\n",
    "        try:    \n",
    "            print 'get prep fn for ', this_desc\n",
    "            #print 'data prep fn: ', models_structure_dict[algorithm][scenario]['data_prep_function']\n",
    "            X,y = models_structure_dict[algorithm][scenario]['data_prep_function'](clinical_data_dfs[algorithm], models_structure_dict[algorithm][scenario])\n",
    "            if scenario == 'age_binned_official': \n",
    "                X = X[sorted(X.columns)]\n",
    "            \n",
    "            apply_output = models_structure_dict[algorithm][scenario]['apply_function'](X, y, models_structure_dict[algorithm][scenario])\n",
    "            \n",
    "            model_response = [x[0] for x in apply_output['model_response']]\n",
    "            model_response_2d = apply_output['model_response']\n",
    "\n",
    "            labels = clinical_data_dfs[algorithm][models_structure_dict[algorithm][scenario]['target']]\n",
    "            clinical_data_dfs[algorithm]['model_response_'+scenario] = model_response\n",
    "            clinical_data_dfs[algorithm]['outcome'] = labels\n",
    "            clinical_data_dfs[algorithm]['classifier_variant'] = algorithm\n",
    "            class_names = sorted(labels.unique())\n",
    "            orig_model_metrics = get_classifier_performance_metrics(class_names, class_priors, labels, apply_output['y_predicted_without_dunno'], apply_output['y_predicted_with_dunno'], apply_output['model_response'])\n",
    "            desired_autism_recall = 0.8\n",
    "            \n",
    "            if scenario == 'engineering_official':   ### Override to use our official results instead\n",
    "                model_response_2d = [[official_response, 1-official_response] for official_response in clinical_data_dfs[algorithm]['Guardian Qnnaire Score'].values]\n",
    "            find_optimal_dunno = True if scenario == 'engineering_official' else False\n",
    "            model_metrics = get_optimized_metrics_from_model_responses(model_response_2d, class_names, class_priors, truth_vals=labels, desired_autism_recall=desired_autism_recall, desc=this_desc, find_optimal_dunno=find_optimal_dunno)\n",
    "            old_clinical_optimized_metric_dict = append_metric_results_to_dict_of_lists(old_clinical_optimized_metric_dict, model_metrics, scenario)\n",
    "\n",
    "\n",
    "        except Exception as this_message:\n",
    "            print 'Problem running algorithm ', algorithm, ', scenario: ', scenario\n",
    "            print 'Error message: ', this_message\n",
    "            for metric_name, metric_details in metrics_to_do.iteritems():\n",
    "                old_clinical_optimized_metric_dict[metric_name][algorithm].append(np.nan)\n",
    "clinical_data_dfs['questionnaire'] = pd.concat([clinical_data_dfs['guardian.qnnaire.3-'], clinical_data_dfs['guardian.qnnaire.4+']], ignore_index=True)\n",
    "clinical_data_dfs['video'] = pd.concat([clinical_data_dfs['video.module.1', 'video.module.2'], ignore_index=True)\n",
    "\n",
    "\n",
    "##### Turn this back off when done testing above\n",
    "\n",
    "old_clinical_metric_dfs = collections.OrderedDict()\n",
    "for metric_name, metric_details in metrics_to_do.iteritems():\n",
    "    print old_clinical_optimized_metric_dict\n",
    "    old_clinical_metric_dfs[metric_name] = pd.DataFrame(old_clinical_optimized_metric_dict[metric_name])\n",
    "    old_clinical_metric_dfs[metric_name].index.names = [metric_details['desc']]\n",
    "    old_clinical_metric_dfs[metric_name]['questionnaire'] = ((2.*old_clinical_metric_dfs[metric_name]['guardian.qnnaire.3-']) +\\\n",
    "                                                        (3.*old_clinical_metric_dfs[metric_name]['guardian.qnnaire.4+'])) / 5.\n",
    "    \n",
    "\n",
    "\n",
    "#clinical_avg_recall_df['scenario'] = clinical_avg_recall_df.index\n",
    "print 'old style clinical_metric_dfs: ', old_clinical_metric_dfs\n",
    "\n",
    "if purpose_of_run == 'for_tables':\n",
    "    make_clinical_latex_tables(old_clinical_metric_dfs, desc='old')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine training data recalls, merge with clinical results, and write output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to explore classification thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_optimized_metrics_dict_from_thresholds_df(thresholds_df, desired_autism_recall, min_coverage=0.7, desc=''):\n",
    "    ### First restric to autism recall within 2% of desired value\n",
    "    thresholds_df['autism_recall'] = thresholds_df['sensitivity']\n",
    "    thresholds_df['not_recall'] = thresholds_df['specificity']\n",
    "    thresholds_df['average_recall'] = 0.5*(thresholds_df['autism_recall']+thresholds_df['not_recall'])\n",
    "    thresholds_df['average_precision_dataset'] = 0.5*(thresholds_df['autism_precision']+thresholds_df['not_precision'])\n",
    "    print 'desc: ', desc, ', autism recalls: ', thresholds_df['autism_recall'].values\n",
    "    restricted_df = thresholds_df[np.abs(thresholds_df['autism_recall'].values-desired_autism_recall)<0.01]\n",
    "    print 'desc: ', desc, ', after requiring good match: ', restricted_df['autism_recall'].values\n",
    "\n",
    "    if len(restricted_df.index)<1:\n",
    "        restricted_df = thresholds_df[np.abs(thresholds_df['autism_recall'].values-desired_autism_recall)<0.02]\n",
    "    if len(restricted_df.index)<1 and desired_specificity_bin_width>0.02:\n",
    "        restricted_df = thresholds_df[np.abs(thresholds_df['autism_recall'].values-desired_autism_recall)<0.04]\n",
    "    assert len(restricted_df.index)>=1\n",
    "\n",
    "\n",
    "#     if len(restricted_df.index)<1:\n",
    "#         restricted_df = thresholds_df[np.abs(thresholds_df['autism_recall'].values-desired_autism_recall)<0.08]\n",
    "    \n",
    "    ### If we are handling a \"dunno\" scenario then apply coverage restriction and return best average recall\n",
    "    if 'coverage' in restricted_df.columns:\n",
    "        restricted_df = restricted_df[restricted_df['coverage']>min_coverage]\n",
    "        print 'After coverage req: ', restricted_df['autism_recall'].values\n",
    "#         optimization_key = 'average_recall'\n",
    "#     else:\n",
    "#         optimization_key = 'AUC'\n",
    "    optimization_key='not_recall'\n",
    "    best_dict = restricted_df.sort_values(optimization_key, ascending=False).reset_index(drop=True).ix[0].to_dict()\n",
    "#     best_results_dict['autism_recall'] = best_results_dict['sensitivity']\n",
    "#     best_results_dict['not_recall'] = best_results_dict['specificity']\n",
    "#     best_results_dict['average_recall'] = 0.5*(best_results_dict['autism_recall']+best_results_dict['not_recall'])\n",
    "#     best_results_dict['average_precision'] = 0.5*(best_results_dict['autism_precision']+best_results_dict['not_precision'])\n",
    "    print 'for ', desc, ', best results are: ', best_dict\n",
    "    return best_dict\n",
    "\n",
    "\n",
    "def explore_classification_thresholds(target_column, response_column, positive_class_name, negative_class_name,\n",
    "                                      desc=''):\n",
    "    \n",
    "    #useless addons that get_classifier_performance_metrics expects but don't apply in this particular case\n",
    "    y_predicted_probs = [ [x, 1.0-x] for x in response_column]\n",
    "    outcome_class_priors = [0.5, 0.5] \n",
    "\n",
    "    thresholds_to_try = np.arange(np.min(response_column), np.max(response_column), (np.max(response_column) - np.min(response_column))/1000.0)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for threshold in thresholds_to_try:\n",
    "\n",
    "        y_predicted_without_dunno = response_column.apply(lambda x: positive_class_name if x>threshold else negative_class_name)\n",
    "        y_predicted_with_dunno = y_predicted_without_dunno\n",
    "\n",
    "        metrics = get_classifier_performance_metrics([positive_class_name, negative_class_name], outcome_class_priors, target_column, y_predicted_without_dunno, y_predicted_with_dunno, y_predicted_probs)\n",
    "\n",
    "        sensitivity = metrics['without_dunno']['dataset_recall_per_class'][positive_class_name]\n",
    "        specificity = metrics['without_dunno']['dataset_recall_per_class'][negative_class_name]\n",
    "        autism_precision = metrics['without_dunno']['dataset_precision_per_class'][positive_class_name]\n",
    "        not_precision = metrics['without_dunno']['dataset_precision_per_class'][negative_class_name]\n",
    "        AUC = cp.deepcopy(metrics['without_dunno']['auc'])\n",
    "        if desc != '' and threshold == thresholds_to_try[0]:\n",
    "            print 'Exploring classification thresholds. For ', desc, ', AUC: ', AUC, '.'\n",
    "\n",
    "        if sensitivity>0.75 and sensitivity<0.85 and desc != '':\n",
    "            print 'Old for ', desc, ', threshold: ', threshold, ', autism recall: ', sensitivity, ', not recall: ', specificity\n",
    "        results+= [[threshold, sensitivity, specificity, autism_precision, not_precision, AUC]]\n",
    "\n",
    "    output = pd.DataFrame(results, columns=['threshold', 'sensitivity', 'specificity', 'autism_precision', 'not_precision', 'AUC'])\n",
    "    return output\n",
    "\n",
    "def explore_dunno_ranges(target_column, response_column, positive_class_name, negative_class_name, desc=''):\n",
    "    \n",
    "    #useless addons that get_classifier_performance_metrics expects but don't apply in this particular case\n",
    "    y_predicted_probs = [ [x, 1.0-x] for x in response_column]\n",
    "    outcome_class_priors = [0.5, 0.5] \n",
    "\n",
    "    dunno_ranges_to_try = []\n",
    "    for lower_value in np.arange(0.00, 1.01, 0.01):\n",
    "        for upper_value in np.arange(lower_value, 1.01, 0.01):\n",
    "            dunno_ranges_to_try += [(lower_value, upper_value)]\n",
    "\n",
    "    results = []\n",
    "    for dunno_range in dunno_ranges_to_try:\n",
    "\n",
    "        y_predicted_with_dunno = response_column.apply(lambda x: positive_class_name if x>dunno_range[1] else (negative_class_name if x<dunno_range[0] else 'dunno' ))\n",
    "        y_predicted_without_dunno = y_predicted_with_dunno\n",
    "        \n",
    "        conclusive_mask = np.array([False if ele=='dunno' else True for ele in y_predicted_with_dunno.values])\n",
    "        conclusive_probs = response_column.values[conclusive_mask]\n",
    "        conclusive_targets = target_column.values[conclusive_mask]\n",
    "        \n",
    "        \n",
    "        metrics = get_classifier_performance_metrics([positive_class_name, negative_class_name], outcome_class_priors, target_column, y_predicted_without_dunno, y_predicted_with_dunno, y_predicted_probs)\n",
    "\n",
    "#         print 'dunno_range: ', dunno_range\n",
    "#         #print 'metrics: ', metrics\n",
    "#         print 'positive_class_name: ', positive_class_name\n",
    "#         print 'negative_class_name: ', negative_class_name\n",
    "\n",
    "#         if 'with_dunno' not in metrics.keys():\n",
    "#             continue\n",
    "#         print 'metrics with dunno: ', metrics['with_dunno']\n",
    "#         print 'classification rate: ', metrics['with_dunno']['dataset_classification_rate']\n",
    "#         print 'recalls: ', metrics['with_dunno']['dataset_recall_per_class_where_classified']\n",
    "        try:\n",
    "            coverage = metrics['with_dunno']['dataset_classification_rate']\n",
    "            sensitivity = metrics['with_dunno']['dataset_recall_per_class_where_classified'][positive_class_name]\n",
    "            specificity = metrics['with_dunno']['dataset_recall_per_class_where_classified'][negative_class_name]\n",
    "            autism_precision = metrics['with_dunno']['dataset_precision_per_class'][positive_class_name]\n",
    "            not_precision = metrics['with_dunno']['dataset_precision_per_class'][negative_class_name]\n",
    "            AUC = metrics['with_dunno']['auc']\n",
    "        except:\n",
    "            coverate = 0\n",
    "            sensitivity = 0\n",
    "            specificity = 0\n",
    "            autism_precision = 0\n",
    "            not_precision = 0\n",
    "            AUC = 0\n",
    "\n",
    "        results+= [[dunno_range[0], dunno_range[1], coverage, sensitivity, specificity, autism_precision, not_precision, AUC]]\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    output = pd.DataFrame(results, columns=['dunno_from', 'dunno_to', 'coverage', 'sensitivity', 'specificity', 'autism_precision', 'not_precision', 'AUC'])\n",
    "    #### peek at some of the best results:\n",
    "    best_grid_search_output = output[(output['coverage']>0.75) & (output['sensitivity']>0.7) & (output['sensitivity']<0.9)].sort_values('AUC', ascending=False)\n",
    "    print 'For explore_dunno_ranges, ', desc, ', best grid search results = '\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    print best_grid_search_output.head(60)\n",
    "    pd.reset_option('display.max_colwidth')\n",
    "    return output\n",
    "\n",
    "def explore_composite_classification_thresholds(dataframe, target_column_name, response_column_name,\n",
    "        classifier_variant_column_name, positive_class_name, negative_class_name, specificity_bin_width = 0.025,\n",
    "        desc=''):\n",
    "    print 'start explore compositve classification thresholds'\n",
    "    determination_column_name = 'determination'\n",
    "    \n",
    "    AUC = metrics.roc_auc_score([ele == 'autism' for ele in dataframe[target_column_name].values], dataframe[response_column_name].values)\n",
    "\n",
    "    variants = []\n",
    "    groups = {}\n",
    "    thresholds = {}\n",
    "    results = []\n",
    "    for variant, group in dataframe.groupby(classifier_variant_column_name):\n",
    "        variants += [variant]\n",
    "        groups[variant] = group\n",
    "    \n",
    "    for variant in variants:\n",
    "        group = groups[variant]\n",
    "        thresholds[variant] = np.arange(np.min(group[response_column_name]), np.max(group[response_column_name]), (np.max(group[response_column_name]) - np.min(group[response_column_name]))/100.0)\n",
    "    threshold_combinations = get_combinations([thresholds[variant] for variant in variants])\n",
    "    \n",
    "    n_threshold_combinations = len(threshold_combinations)\n",
    "    print_level = n_threshold_combinations / 100\n",
    "    for tidx, threshold_combination in enumerate(threshold_combinations):\n",
    "        if tidx % print_level == 0:\n",
    "            print 'start threshold combination ', tidx, ' of ', n_threshold_combinations\n",
    "        \n",
    "        total_true_positives = 0\n",
    "        total_true_negatives = 0\n",
    "        total_false_positives = 0\n",
    "        total_false_negatives = 0\n",
    "        for i in range(0, len(variants)):\n",
    "            variant = variants[i]\n",
    "            threshold = threshold_combination[i]\n",
    "            group = groups[variant]\n",
    "            group[determination_column_name] = group.apply(lambda row: positive_class_name if row[response_column_name]>threshold else negative_class_name, axis=1)\n",
    "            \n",
    "            true_positives = len(group[(group[determination_column_name]==positive_class_name) & (group[target_column_name]==positive_class_name)])\n",
    "            true_negatives = len(group[(group[determination_column_name]==negative_class_name) & (group[target_column_name]==negative_class_name)])\n",
    "            false_positives = len(group[(group[determination_column_name]==positive_class_name) & (group[target_column_name]!=positive_class_name)])\n",
    "            false_negatives = len(group[(group[determination_column_name]==negative_class_name) & (group[target_column_name]!=negative_class_name)])\n",
    "            \n",
    "            total_true_positives += true_positives\n",
    "            total_true_negatives += true_negatives\n",
    "            total_false_positives += false_positives\n",
    "            total_false_negatives += false_negatives\n",
    "            \n",
    "        sensitivity = float(total_true_positives) / float(total_true_positives+total_false_negatives)\n",
    "        specificity = float(total_true_negatives) / float(total_true_negatives+total_false_positives)\n",
    "        autism_precision = float(total_true_positives) / float(total_true_positives+total_false_positives)\n",
    "        not_precision = float(total_true_negatives) / float(total_true_negatives+total_false_negatives)\n",
    "\n",
    "        results +=  [[dict(zip(variants, threshold_combination)), sensitivity, specificity, autism_precision, not_precision, AUC]]\n",
    "        \n",
    "    grid_search_output = pd.DataFrame(results, columns=['threshold_combination', 'sensitivity', 'specificity', 'autism_precision', 'not_precision', 'AUC'])\n",
    "    grid_search_output\n",
    "\n",
    "    grid_search_output['rounded_specificity'] = grid_search_output['specificity'].apply(lambda x: 0 if np.isnan(x) else specificity_bin_width*(int(x/specificity_bin_width)) )\n",
    "#     grid_search_output['rounded_autism_precision'] = grid_search_output['autism_precision'].apply(lambda x: 0 if np.isnan(x) else specificity_bin_width*(int(x/specificity_bin_width)))\n",
    "#     grid_search_output['rounded_not_precision'] = grid_search_output['not_precision'].apply(lambda x: 0 if np.isnan(x) else specificity_bin_width*(int(x/specificity_bin_width)))\n",
    "\n",
    "    sensitivity = grid_search_output.groupby('rounded_specificity')['sensitivity'].max()\n",
    "    specificity = grid_search_output.groupby('rounded_specificity')['rounded_specificity'].max()\n",
    "    print 'sensitivity: ', sensitivity\n",
    "    print 'specificity: ', specificity\n",
    "    print 'grid_search_output of rounded specificity: ', grid_search_output['rounded_specificity']\n",
    "    autism_precision = [grid_search_output[(grid_search_output['rounded_specificity']==this_specificity) & (grid_search_output['sensitivity']==this_sensitivity)]['autism_precision'].values[0] for this_specificity, this_sensitivity in zip(specificity.values, sensitivity.values)]\n",
    "    print 'autism_precision: ', autism_precision\n",
    "    not_precision = [grid_search_output[(grid_search_output['rounded_specificity']==this_specificity) & (grid_search_output['sensitivity']==this_sensitivity)]['not_precision'].values[0] for this_specificity, this_sensitivity in zip(specificity.values, sensitivity.values)]\n",
    "    AUC_arr = [grid_search_output[(grid_search_output['rounded_specificity']==this_specificity) & (grid_search_output['sensitivity']==this_sensitivity)]['AUC'].values[0] for this_specificity, this_sensitivity in zip(specificity.values, sensitivity.values)]\n",
    "    print 'not_precision: ', not_precision\n",
    "#     autism_precision = grid_search_output.groupby('rounded_autism_precision')['rounded_autism_precision'].max()\n",
    "#     autism_precision = grid_search_output.groupby('rounded_autism_precision')['rounded_autism_precision'].max()\n",
    "\n",
    "\n",
    "    output = pd.DataFrame(zip(sensitivity, specificity, autism_precision, not_precision, AUC_arr), columns=['sensitivity', 'specificity', 'autism_precision', 'not_precision', 'AUC'])\n",
    "    return output\n",
    "\n",
    "\n",
    "def explore_composite_classification_dunno_ranges(dataframe, target_column_name, response_column_name,\n",
    "        classifier_variant_column_name, positive_class_name, negative_class_name, specificity_bin_width = 0.025,\n",
    "        coverage_bin_width=0.025, desc=''):\n",
    "    def is_conclusive(response, dunno_range):\n",
    "        if dunno_range[0] <= response <= dunno_range[1]: \n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    \n",
    "    print 'Begin exploring composite classification dunno ranges'\n",
    "    determination_column_name = 'determination'\n",
    "\n",
    "    AUC = metrics.roc_auc_score([ele == 'autism' for ele in dataframe[target_column_name].values], dataframe[response_column_name].values)\n",
    "\n",
    "    variants = []\n",
    "    groups = {}\n",
    "    dunno_ranges = {}\n",
    "    results = []\n",
    "    for variant, group in dataframe.groupby(classifier_variant_column_name):\n",
    "        variants += [variant]\n",
    "        groups[variant] = group\n",
    "    #print 'classifier_variant_column_name: ', classifier_variant_column_name\n",
    "    #print 'values: ', dataframe[classifier_variant_column_name]\n",
    "    #print 'variants: ', variants\n",
    "    \n",
    "    for variant in variants:\n",
    "        group = groups[variant]\n",
    "        dunno_ranges[variant] = []\n",
    "        for lower_value in np.arange(0.00, 1.1, 0.1):\n",
    "            for upper_value in np.arange(lower_value, 1.1, 0.1):\n",
    "                dunno_ranges[variant] += [(lower_value, upper_value)]\n",
    "    dunno_combinations = get_combinations([dunno_ranges[variant] for variant in variants])\n",
    "\n",
    "\n",
    "    n_threshold_combinations = len(dunno_combinations)\n",
    "    print 'n_threshold combinations: ', n_threshold_combinations\n",
    "    print_level = n_threshold_combinations / 500\n",
    "    for didx, dunno_combination in enumerate(dunno_combinations):\n",
    "                \n",
    "        do_debug = False\n",
    "#         if didx % print_level == 0:\n",
    "#             print 'start dunno combination ', didx, ' of ', n_threshold_combinations\n",
    "#             do_debug = True\n",
    "        total_inconclusives = 0\n",
    "        total_true_positives = 0\n",
    "        total_true_negatives = 0\n",
    "        total_false_positives = 0\n",
    "        total_false_negatives = 0\n",
    "\n",
    "        #print 'Get conclusive mask for dunno_combination: ', dunno_combination\n",
    "        #print 'responses: ', dataframe[response_column_name].values\n",
    "        #print 'Variants: ', dataframe[classifier_variant_column_name].values\n",
    "        variant_indices = [variants.index(classifier_variant) for classifier_variant in dataframe[classifier_variant_column_name].values]\n",
    "        #print 'variant_indices: ', variant_indices\n",
    "        conclusive_mask = np.array([True if is_conclusive(response, dunno_combination[variant_index]) else False for variant_index, response in zip(variant_indices, dataframe[response_column_name].values)])\n",
    "        #print 'Conclusive mask: ', conclusive_mask\n",
    "        target_where_classified = dataframe[target_column_name].values[conclusive_mask]\n",
    "        #print 'target_where_classified: ', target_where_classified\n",
    "        response_where_classified = dataframe[response_column_name].values[conclusive_mask]\n",
    "        #print 'response_where_classified: ', response_where_classified\n",
    "        try:\n",
    "            dunno_AUC = metrics.roc_auc_score([ele == 'autism' for ele in target_where_classified], response_where_classified)\n",
    "        except:\n",
    "            dunno_AUC = 0\n",
    "        #print 'dunno_AUC: ', dunno_AUC\n",
    "\n",
    "        \n",
    "        for i in range(0, len(variants)):\n",
    "            variant = variants[i]\n",
    "            dunno_range = dunno_combination[i]\n",
    "            group = groups[variant]\n",
    "            group[determination_column_name] = group.apply(lambda row: positive_class_name if row[response_column_name]>dunno_range[1] else negative_class_name if row[response_column_name]<dunno_range[0] else 'dunno', axis=1)\n",
    "            \n",
    "            inconclusives = len(group[(group[determination_column_name]=='dunno')])\n",
    "            true_positives = len(group[(group[determination_column_name]==positive_class_name) & (group[target_column_name]==positive_class_name)])\n",
    "            true_negatives = len(group[(group[determination_column_name]==negative_class_name) & (group[target_column_name]==negative_class_name)])\n",
    "            false_positives = len(group[(group[determination_column_name]==positive_class_name) & (group[target_column_name]!=positive_class_name)])\n",
    "            false_negatives = len(group[(group[determination_column_name]==negative_class_name) & (group[target_column_name]!=negative_class_name)])\n",
    "            \n",
    "            total_inconclusives += inconclusives\n",
    "            total_true_positives += true_positives\n",
    "            total_true_negatives += true_negatives\n",
    "            total_false_positives += false_positives\n",
    "            total_false_negatives += false_negatives\n",
    "            \n",
    "            if do_debug:\n",
    "                print 'dunno_range: ', dunno_range\n",
    "                print 'variant: ', variants[i]\n",
    "                print 'group: ', list(group[determination_column_name].values)\n",
    "                print 'total_inconclusives: ', total_inconclusives\n",
    "                print 'total_true_positives: ', total_true_positives\n",
    "                print 'total_true_negatives: ', total_true_negatives\n",
    "                print 'total_false_positives: ', total_false_positives\n",
    "                print 'total_false_negatives: ', total_false_negatives\n",
    "\n",
    "\n",
    "        \n",
    "        inconclusive_rate = float(inconclusives) / float(inconclusives+total_true_positives+total_false_negatives+total_true_negatives+total_false_positives)\n",
    "        coverage = 1.0 - inconclusive_rate\n",
    "        sensitivity = 0 if float(total_true_positives+total_false_negatives)==0 else float(total_true_positives) / float(total_true_positives+total_false_negatives)\n",
    "        specificity = 0 if float(total_true_negatives+total_false_positives)==0 else float(total_true_negatives) / float(total_true_negatives+total_false_positives)\n",
    "        autism_precision = 0 if float(total_true_positives+total_false_positives)==0 else float(total_true_positives) / float(total_true_positives+total_false_positives)\n",
    "        not_precision = 0 if float(total_true_negatives+total_false_negatives)==0 else float(total_true_negatives) / float(total_true_negatives+total_false_negatives)\n",
    "\n",
    "        if do_debug:\n",
    "            print 'dunno_range: ', dunno_range\n",
    "            print 'coverage: ', coverage\n",
    "            print 'sensitivity: ', sensitivity\n",
    "            print 'specificity: ', specificity\n",
    "        results +=  [[dict(zip(variants, dunno_combination)), coverage, sensitivity, specificity, autism_precision, not_precision, dunno_AUC]]\n",
    "        \n",
    "    grid_search_output = pd.DataFrame(results, columns=['threshold_combination', 'coverage', 'sensitivity', 'specificity', 'autism_precision', 'not_precision', 'AUC'])\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    \n",
    "    #### peek at some of the best results:\n",
    "    best_grid_search_output = grid_search_output[(grid_search_output['coverage']>0.75) & (grid_search_output['sensitivity']>0.7) & (grid_search_output['sensitivity']<0.9)].sort_values('AUC', ascending=False)\n",
    "    print 'For explore_composite_classification_dunno_ranges, ', desc, ', best grid search results = '\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    print best_grid_search_output.head(60)\n",
    "    pd.reset_option('display.max_colwidth')\n",
    "    \n",
    "    grid_search_output['rounded_specificity'] = grid_search_output['specificity'].apply(lambda x: 0 if np.isnan(x) else specificity_bin_width*(int(x/specificity_bin_width)) )\n",
    "    #print 'grid search coverage: ', list(grid_search_output['coverage'].values)\n",
    "    grid_search_output['rounded_coverage'] = grid_search_output['coverage'].apply(lambda x: 0 if np.isnan(x) else coverage_bin_width*(int(x/coverage_bin_width)) )\n",
    "    #print 'grid_search_output: ', grid_search_output\n",
    "    #print 'rounded coverage pre grouping: ', list(grid_search_output['rounded_coverage'].values)\n",
    "    \n",
    "    sensitivity = grid_search_output.groupby([ 'rounded_coverage','rounded_specificity'])['sensitivity'].max()\n",
    "    coverage = grid_search_output.groupby(['rounded_coverage', 'rounded_specificity' ])['rounded_coverage'].max()\n",
    "    specificity = grid_search_output.groupby(['rounded_coverage', 'rounded_specificity'])['rounded_specificity'].max()\n",
    "#     autism_precision = grid_search_output[grid_search_output['rounded_specificity']==specificity]['autism_precision'].values[0]\n",
    "    autism_precision = [grid_search_output[(grid_search_output['rounded_specificity']==this_specificity) & (grid_search_output['sensitivity']==this_sensitivity)]['autism_precision'].values[0] for this_specificity, this_sensitivity in zip(specificity.values, sensitivity.values)]\n",
    "#     not_precision = grid_search_output[grid_search_output['rounded_specificity']==specificity]['not_precision'].values[0]\n",
    "    not_precision = [grid_search_output[(grid_search_output['rounded_specificity']==this_specificity) & (grid_search_output['sensitivity']==this_sensitivity)]['not_precision'].values[0] for this_specificity, this_sensitivity in zip(specificity.values, sensitivity.values)]\n",
    "    AUC_arr = [grid_search_output[(grid_search_output['rounded_specificity']==this_specificity) & (grid_search_output['sensitivity']==this_sensitivity)]['AUC'].values[0] for this_specificity, this_sensitivity in zip(specificity.values, sensitivity.values)]\n",
    "\n",
    "    #print 'coverage groups: '\n",
    "    #for group, data in grid_search_output.groupby(['rounded_coverage', 'rounded_specificity' ])['rounded_coverage']:\n",
    "    #    print 'group: ', group\n",
    "    #    print 'data: ', data\n",
    "    #print 'rounded_coverage: ', coverage\n",
    "    #print 'rounded sensitivity: ', sensitivity\n",
    "    #print 'rounded specificity: ', specificity\n",
    "    output = pd.DataFrame(zip(coverage, sensitivity, specificity, autism_precision, not_precision, AUC_arr), columns=['coverage', 'sensitivity', 'specificity', 'autism_precision', 'not_precision', 'AUC'])\n",
    "    \n",
    "    return output\n",
    "\n",
    "# def explore_composite_classification_dunno_ranges(dataframe, target_column_name, response_column_name, classifier_variant_column_name, positive_class_name, negative_class_name, specificity_bin_width = 0.025, coverage_bin_width=0.025):\n",
    "    \n",
    "#     determination_column_name = 'determination'\n",
    "\n",
    "\n",
    "#     variants = []\n",
    "#     groups = {}\n",
    "#     dunno_ranges = {}\n",
    "#     results = []\n",
    "#     for variant, group in dataframe.groupby(classifier_variant_column_name):\n",
    "#         variants += [variant]\n",
    "#         groups[variant] = group\n",
    "    \n",
    "#     for variant in variants:\n",
    "#         group = groups[variant]\n",
    "#         dunno_ranges[variant] = []\n",
    "#         for lower_value in np.arange(0.00, 1.1, 0.1):\n",
    "#             for upper_value in np.arange(lower_value, 1.1, 0.1):\n",
    "#                 dunno_ranges[variant] += [(lower_value, upper_value)]\n",
    "#     dunno_combinations = get_combinations([dunno_ranges[variant] for variant in variants])\n",
    "\n",
    "\n",
    "#     print_level = len(dunno_combinations) / 100\n",
    "\n",
    "\n",
    "#     for didx, dunno_combination in enumerate(dunno_combinations):\n",
    "#         do_debug = False\n",
    "#         if didx %  print_level == 0:\n",
    "#             print 'start dunno combination ', didx, ' of ', len(dunno_combinations)\n",
    "#             do_debug = True\n",
    "\n",
    "#         total_inconclusives = 0\n",
    "#         total_true_positives = 0\n",
    "#         total_true_negatives = 0\n",
    "#         total_false_positives = 0\n",
    "#         total_false_negatives = 0\n",
    "#         for i in range(0, len(variants)):\n",
    "#             variant = variants[i]\n",
    "#             dunno_range = dunno_combination[i]\n",
    "#             group = groups[variant]\n",
    "#             group[determination_column_name] = group.apply(lambda row: positive_class_name if row[response_column_name]>dunno_range[1] else negative_class_name if row[response_column_name]<dunno_range[0] else 'dunno', axis=1)\n",
    "            \n",
    "#             inconclusives = len(group[(group[determination_column_name]=='dunno')])\n",
    "#             true_positives = len(group[(group[determination_column_name]==positive_class_name) & (group[target_column_name]==positive_class_name)])\n",
    "#             true_negatives = len(group[(group[determination_column_name]==negative_class_name) & (group[target_column_name]==negative_class_name)])\n",
    "#             false_positives = len(group[(group[determination_column_name]==positive_class_name) & (group[target_column_name]!=positive_class_name)])\n",
    "#             false_negatives = len(group[(group[determination_column_name]==negative_class_name) & (group[target_column_name]!=negative_class_name)])\n",
    "            \n",
    "#             total_inconclusives += inconclusives\n",
    "#             total_true_positives += true_positives\n",
    "#             total_true_negatives += true_negatives\n",
    "#             total_false_positives += false_positives\n",
    "#             total_false_negatives += false_negatives\n",
    "            \n",
    "#             if do_debug:\n",
    "#                 print 'dunno_range: ', dunno_range\n",
    "#                 print 'variant: ', variants[i]\n",
    "#                 print 'group: ', list(group[determination_column_name].values)\n",
    "#                 print 'total_inconclusives: ', total_inconclusives\n",
    "#                 print 'total_true_positives: ', total_true_positives\n",
    "#                 print 'total_true_negatives: ', total_true_negatives\n",
    "#                 print 'total_false_positives: ', total_false_positives\n",
    "#                 print 'total_false_negatives: ', total_false_negatives\n",
    "\n",
    "        \n",
    "#         inconclusive_rate = float(inconclusives) / float(inconclusives+total_true_positives+total_false_negatives+total_true_negatives+total_false_positives)\n",
    "#         coverage = 1.0 - inconclusive_rate\n",
    "#         sensitivity = 0 if float(total_true_positives+total_false_negatives)==0 else float(total_true_positives) / float(total_true_positives+total_false_negatives)\n",
    "#         specificity = 0 if float(total_true_negatives+total_false_positives)==0 else float(total_true_negatives) / float(total_true_negatives+total_false_positives)\n",
    "        \n",
    "#         if do_debug:\n",
    "#             print 'dunno_range: ', dunno_range\n",
    "#             print 'coverage: ', coverage\n",
    "#             print 'sensitivity: ', sensitivity\n",
    "#             print 'specificity: ', specificity\n",
    "\n",
    "#         results +=  [[dict(zip(variants, dunno_combination)), coverage, sensitivity, specificity]]\n",
    "        \n",
    "#     grid_search_output = pd.DataFrame(results, columns=['threshold_combination', 'coverage', 'sensitivity', 'specificity',])\n",
    "#     grid_search_output\n",
    "    \n",
    "#     grid_search_output['rounded_specificity'] = grid_search_output['specificity'].apply(lambda x: 0 if np.isnan(x) else specificity_bin_width*(int(x/specificity_bin_width)) )\n",
    "#     grid_search_output['rounded_coverage'] = grid_search_output['coverage'].apply(lambda x: 0 if np.isnan(x) else coverage_bin_width*(int(x/coverage_bin_width)) )\n",
    "\n",
    "#     sensitivity = grid_search_output.groupby([ 'rounded_coverage','rounded_specificity'])['sensitivity'].max()\n",
    "#     coverage = grid_search_output.groupby(['rounded_coverage', 'rounded_specificity' ])['rounded_coverage'].max()\n",
    "#     specificity = grid_search_output.groupby(['rounded_coverage', 'rounded_specificity'])['rounded_specificity'].max()\n",
    "\n",
    "#     output = pd.DataFrame(zip(coverage, sensitivity, specificity), columns=['coverage', 'sensitivity', 'specificity'])\n",
    "#     return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make questionnaire ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### Here we will fill in results for the clinical questionnaire performance tables in the paper\n",
    "clinical_optimized_metric_dict = collections.OrderedDict()\n",
    "for metric_name, metric_details in metrics_to_do.iteritems():\n",
    "    clinical_optimized_metric_dict[metric_name] = collections.OrderedDict(\n",
    "        (algorithm, []) for algorithm in algorithms.keys()+['questionnaire']\n",
    "    )\n",
    "\n",
    "for metric_name, metric_details in metrics_to_do.iteritems():\n",
    "    clinical_optimized_metric_dict[metric_name]['scenario'] = clinical_scenarios + ['engineering_official_dunno']\n",
    "    clinical_optimized_metric_dict[metric_name]['latex_scenario'] = [value['latex_desc'] for value in clinical_scenarios_dict.values()]+['with 75\\\\% coverage']\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 300)\n",
    "print 'relevant columns:'\n",
    "print clinical_data_dfs['questionnaire'][['Patient Id', 'classifier_variant', 'Guardian Qnnaire Score', 'outcome']].sort_values(['Patient Id']).reset_index()\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "print 'mchat_df: ', mchat_df['Patient Id']\n",
    "print 'cdcl_df: ', cbcl_df['Patient Id']\n",
    "print 'clinical_data_dfs keys: ', clinical_data_dfs.keys()\n",
    "\n",
    "\n",
    "\n",
    "print 'before filter on ids, len(mchat df): ', len(mchat_df.index), ', cbcl: ', len(cbcl_df.index)\n",
    "clinical_ids = clinical_data_dfs['questionnaire']['Patient Id'].values\n",
    "mchat_df = mchat_df[mchat_df['Patient Id'].isin(clinical_ids)]\n",
    "cbcl_df = cbcl_df[cbcl_df['Patient Id'].isin(clinical_ids)]\n",
    "print 'after filter on ids, len(mchat df): ', len(mchat_df.index), ', cbcl: ', len(cbcl_df.index)\n",
    "\n",
    "\n",
    "for algorithm in algorithms.keys():\n",
    "    if algorithm in ['video.module.1', 'video.module.2', 'video']: \n",
    "        continue\n",
    "    data_to_plot = []\n",
    "    ages_desc = 'Age $< 4$ yr, ' if ages_dict[algorithm] == 'young' else 'Age $\\\\geq 4$ yr'\n",
    "\n",
    "\n",
    "    for scenario in clinical_scenarios+['model']:\n",
    "        this_desc = algorithm + '_' + scenario\n",
    "        this_N = len(clinical_data_dfs[algorithm].index)\n",
    "        if scenario == 'engineering_official':   ### We'll show official selection results in the ROC curves instead\n",
    "            continue\n",
    "        \n",
    "        if scenario == 'model':\n",
    "            scenario_desc = 'Questionnaire'\n",
    "            response_str = 'Guardian Qnnaire Score'\n",
    "        else:\n",
    "            scenario_desc = clinical_scenarios_dict[scenario]['latex_desc']\n",
    "            #this_desc  = algorithm + '_' + clinical_scenarios_dict[scenario]['latex_desc']\n",
    "            #print 'make plots for ', this_desc\n",
    "            response_str = 'model_response_'+scenario\n",
    "        \n",
    "        if scenario == 'model':\n",
    "            color = 'orange'\n",
    "            linestyle = 'dashdot'\n",
    "            linewidth = 4\n",
    "        else:\n",
    "            color = clinical_scenarios_dict[scenario]['color']\n",
    "            linestyle = clinical_scenarios_dict[scenario]['linestyle']\n",
    "            linewidth = clinical_scenarios_dict[scenario]['linewidth']\n",
    "        classifier_info = {'label': ages_desc+', '+scenario_desc, 'color': color,\n",
    "                           'linestyle': linestyle, 'linewidth': linewidth}\n",
    "        classifier_data = explore_classification_thresholds(clinical_data_dfs[algorithm]['outcome'],\n",
    "                            clinical_data_dfs[algorithm][response_str], 'autism', 'not', desc=algorithm+'_'+scenario)\n",
    "        print 'For desc: ', this_desc, ', classifier_data: ', classifier_data\n",
    "        print 'For desc: ', this_desc, ', AUC details: ', classifier_data['AUC'].values\n",
    "        best_results_dict = get_optimized_metrics_dict_from_thresholds_df(classifier_data, desired_autism_recall=0.8,\n",
    "                                                                    min_coverage=0.75, desc=algorithm+'_'+scenario)\n",
    "        for metric_name, metric_details in metrics_to_do.iteritems():\n",
    "            metric_value = best_results_dict[metric_name]\n",
    "            clinical_optimized_metric_dict[metric_name][algorithm].append(metric_value)\n",
    "            print 'For ', this_desc, ', metric: ', metric_name, ', append ', metric_value\n",
    "        print 'For ', this_desc, ', clinical_optimized_metric_dict is now ', clinical_optimized_metric_dict\n",
    "            \n",
    "   \n",
    "        data_to_plot.append((classifier_info, classifier_data))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        if scenario == 'model':\n",
    "            dunno_classifier_info = {'label': ages_desc+', '+scenario_desc, 'color': 'black',\n",
    "                    'linewidth': linewidth, 'linestyle': 'solid', 'coverage': 0.75}\n",
    "            dunno_classifier_plotting_data = explore_dunno_ranges(clinical_data_dfs[algorithm]['outcome'],\n",
    "                    clinical_data_dfs[algorithm][response_str], 'autism', 'not', desc=algorithm+'_'+scenario)\n",
    "            data_to_plot.append((dunno_classifier_info, dunno_classifier_plotting_data))\n",
    "            best_results_dict = get_optimized_metrics_dict_from_thresholds_df(dunno_classifier_plotting_data, desired_autism_recall=0.8,\n",
    "                                        min_coverage=0.75, desc=algorithm+'_'+scenario)\n",
    "            print 'For model scenario, algorithm: ', algorithm, ', best_results_dict: ', best_results_dict\n",
    "            for metric_name, metric_details in metrics_to_do.iteritems():\n",
    "                metric_value = best_results_dict.get(metric_name, np.nan)\n",
    "                print 'For modle scenario, dunno case, algorithm: ', algorithm, ', metric: ', metric_name, ', metric_value: ', metric_value\n",
    "                clinical_optimized_metric_dict[metric_name][algorithm].append(metric_value)\n",
    "\n",
    "    title = ''\n",
    "    plt, fig = plot_classifier_profiles( data_to_plot , title, ylim=(0.4,1.), legend_font_size=14)\n",
    "    if purpose_of_run == 'for_plotting':\n",
    "        filename = 'images/'+algorithm+'_ROC_curves.png'\n",
    "    else:\n",
    "        filename = 'images/'+algorithm+'_ROC_curves_fine_binned.png'\n",
    "    fig.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    \n",
    "    \n",
    "print 'Now do combined plots. Pre-combination, optimized metrics dict looks like this: '\n",
    "print clinical_optimized_metric_dict\n",
    "classifier_variant_column_name = 'classifier_variant'\n",
    "\n",
    "\n",
    "mchat_df_to_plot = mchat_df[mchat_df['Mchat Final Score']>=0]\n",
    "### Merge to get the needed outcome results\n",
    "mchat_df_to_plot = mchat_df_to_plot.merge(clinical_data_dfs['questionnaire'], on='Patient Id', how='left', suffixes=('','_dummy'))\n",
    "mchat_info = {'label': \"MCHAT\", 'color': 'darkgrey', 'linestyle': 'dashed', 'linewidth': 3}\n",
    "mchat_plotting_data = explore_classification_thresholds(mchat_df_to_plot['outcome'], mchat_df_to_plot['Mchat Final Score'], 'autism', 'not')\n",
    "cbcl_df_to_plot = cbcl_df[cbcl_df['CBCL Score']>=0]\n",
    "cbcl_df_to_plot = cbcl_df_to_plot.merge(clinical_data_dfs['questionnaire'], on='Patient Id', how='left', suffixes=('','_dummy'))\n",
    "cbcl_info = {'label': \"CBCL\", 'color': 'lightgrey', 'linestyle': 'dotted', 'linewidth': 3}\n",
    "cbcl_plotting_data = explore_classification_thresholds(cbcl_df_to_plot['outcome'], cbcl_df_to_plot['CBCL Score'], 'autism', 'not')\n",
    "\n",
    "data_to_plot = [(mchat_info, mchat_plotting_data), (cbcl_info, cbcl_plotting_data)]\n",
    "\n",
    "\n",
    "all_scenario_plotting_dfs = collections.OrderedDict()\n",
    "for scenario in clinical_scenarios+['model']:\n",
    "    if scenario == 'engineering_official':   ### We'll show official selection results in the ROC curves instead\n",
    "        continue\n",
    "        \n",
    "    if scenario == 'model':\n",
    "        scenario_desc = 'Aggregate features variant'\n",
    "        response_str = 'Guardian Qnnaire Score'\n",
    "    else:\n",
    "        scenario_desc = clinical_scenarios_dict[scenario]['latex_desc']    \n",
    "        response_str = 'model_response_'+scenario\n",
    "    \n",
    "    if scenario == 'model':\n",
    "        color = 'orange'\n",
    "        linestyle = 'dashdot'\n",
    "        linewidth = 4\n",
    "    else:\n",
    "        color = clinical_scenarios_dict[scenario]['color']\n",
    "        linestyle = clinical_scenarios_dict[scenario]['linestyle']\n",
    "        linewidth = clinical_scenarios_dict[scenario]['linewidth']\n",
    "    \n",
    "    this_N = len(clinical_data_dfs['questionnaire'].index)\n",
    "    classifier_info = {'label': scenario_desc, 'color': color,\n",
    "                      'linestyle': linestyle, 'linewidth': linewidth}\n",
    "\n",
    "    classifier_data = explore_composite_classification_thresholds(clinical_data_dfs['questionnaire'], 'outcome',\n",
    "                response_str, classifier_variant_column_name, 'autism', 'not', specificity_bin_width=desired_specificity_bin_width)\n",
    "    if scenario=='model':\n",
    "        print 'For model, composite classifier info: ', classifier_info\n",
    "        pd.set_option('display.max_rows', 50000) \n",
    "        pd.set_option('display.max_columns', 50000)                \n",
    "        print 'For model, composite classifier data: ', classifier_data \n",
    "        print 'Arose from df ', clinical_data_dfs['questionnaire']\n",
    "        pd.set_option('display.max_rows', 500)\n",
    "        pd.set_option('display.max_columns', 50)                \n",
    "\n",
    "\n",
    "    best_results_dict = get_optimized_metrics_dict_from_thresholds_df(classifier_data, desired_autism_recall=0.8,\n",
    "                                                                    min_coverage=0.75, desc=algorithm+'_'+scenario)\n",
    "    print 'combined optimized thresholds dict results: ', best_results_dict\n",
    "    for metric_name, metric_details in metrics_to_do.iteritems():\n",
    "        metric_value = best_results_dict.get(metric_name, np.nan)\n",
    "        clinical_optimized_metric_dict[metric_name]['questionnaire'].append(metric_value)\n",
    "    print 'for ', (algorithm+'_'+scenario), ', plot classifier_data: ', classifier_data\n",
    "    data_to_plot.append((classifier_info, classifier_data))\n",
    "    \n",
    "\n",
    "    if scenario == 'model':\n",
    "        \n",
    "        dunno_classifier_info = {'label': 'Inconclusive results variant', 'color': 'black',\n",
    "                'linestyle': 'solid', 'linewidth': 5}\n",
    "        dunno_classifier_plotting_data = explore_composite_classification_dunno_ranges(clinical_data_dfs['questionnaire'],\n",
    "                'outcome', 'Guardian Qnnaire Score', classifier_variant_column_name, 'autism', 'not', \n",
    "                specificity_bin_width=desired_specificity_bin_width, desc='questionnaire model')\n",
    "        pd.set_option('display.max_rows', 500)\n",
    "        print 'dunno_classifier_plotting_data: ', dunno_classifier_plotting_data\n",
    "        data_to_plot.append((dunno_classifier_info, dunno_classifier_plotting_data))\n",
    "        best_results_dict = get_optimized_metrics_dict_from_thresholds_df(dunno_classifier_plotting_data, desired_autism_recall=0.8,\n",
    "                                        min_coverage=0.75, desc=algorithm+'_'+scenario)\n",
    "        print 'For model scenario, best_results_dict: ', best_results_dict\n",
    "        for metric_name, metric_details in metrics_to_do.iteritems():\n",
    "            metric_value = best_results_dict.get(metric_name, np.nan)\n",
    "            print 'for model scenario, append metric for questionairre, metric: ', metric_name, ', value: ', metric_value\n",
    "            clinical_optimized_metric_dict[metric_name]['questionnaire'].append(metric_value)\n",
    "        print 'After adding in model scenario, optimized metric dict is ', clinical_optimized_metric_dict\n",
    "\n",
    "\n",
    "    #def explore_composite_classification_dunno_ranges(dataframe, target_column_name, response_column_name, classifier_variant_column_name, positive_class_name, negative_class_name, specificity_bin_width = 0.025, coverage_bin_width=0.025):\n",
    "\n",
    "        print 'plot classifier profiles for scenario: ', scenario\n",
    "        print 'len of data to plot: ', len(data_to_plot)\n",
    "        print 'data_to_plot: ', data_to_plot\n",
    "        #title = \"Sensitivity-Specificity Tradeoff for questionnaire\"\n",
    "        title = ''\n",
    "        plt, fig = plot_classifier_profiles( data_to_plot , title, ylim=(0.4,1.))\n",
    "        if purpose_of_run == 'for_plotting':\n",
    "            png_filename = 'images/'+scenario+'_questionnaire_ROC.png'\n",
    "        else:\n",
    "            png_filename = 'images/'+scenario+'_questionnaire_ROC_fine_binned.png'\n",
    "        print 'save file with name: ', png_filename\n",
    "\n",
    "fig.savefig(png_filename, bbox_inches='tight', pad_inches=0)\n",
    "   \n",
    "\n",
    "clinical_metric_dfs = collections.OrderedDict()\n",
    "for metric_name, metric_details in metrics_to_do.iteritems():\n",
    "    print 'Now make dataframes for use in tables from clinical_optimized_metric_dict: ', clinical_optimized_metric_dict\n",
    "    clinical_metric_dfs[metric_name] = pd.DataFrame(clinical_optimized_metric_dict[metric_name])\n",
    "    clinical_metric_dfs[metric_name].index.names = [metric_details['desc']]\n",
    "if purpose_of_run == 'for_tables':\n",
    "    pd.set_option('display.max_columns', 200)\n",
    "    print 'make clinical latex tables for ', clinical_metric_dfs\n",
    "    make_clinical_latex_tables(clinical_metric_dfs, desc='new')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now load the video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(input_video_data, 'rU')\n",
    "dictReader = csv.DictReader(f, delimiter=',')\n",
    "data = []\n",
    "for row in dictReader:\n",
    "    data.append(row)\n",
    "\n",
    "clinical_video_df = pd.DataFrame(data)\n",
    "clinical_video_df.shape\n",
    "\n",
    "clinical_video_df['Video Score'] = clinical_video_df['response'].apply(float)\n",
    "clinical_video_df['Patient Id'] = clinical_video_df['Patient Id'].astype(int)\n",
    "\n",
    "print clinical_video_df.head(1)\n",
    "\n",
    "print clinical_video_df['Video Version'].value_counts()\n",
    "\n",
    "print 'Patient Ids in the video dfs: ', list(clinical_video_df['Patient Id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_missing_value(value):\n",
    "    try:\n",
    "        if value is None or np.isnan(value) or value == 'None' or value == 'NaN':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "combined_clinical_df = clinical_data_dfs['questionnaire'].merge(clinical_video_df, on='Patient Id', how='inner', suffixes=('_qnnaire','_video'))\n",
    "print 'combined_clinical_df: ', combined_clinical_df\n",
    "\n",
    "combined_clinical_df['outcome'] = [qnnaire_outcome if is_missing_value(video_outcome) else video_outcome for qnnaire_outcome,\n",
    "        video_outcome in zip(combined_clinical_df['outcome_qnnaire'].values, combined_clinical_df['outcome_video'].values)]\n",
    "\n",
    "print 'resulting outcome: ', list(combined_clinical_df['outcome'].values)\n",
    "\n",
    "combined_clinical_df = combined_clinical_df.merge(mchat_df, left_on='Patient Id', right_on='Patient Id', how='left', suffixes=('','_mchat'))\n",
    "combined_clinical_df = combined_clinical_df.merge(cbcl_df, left_on='Patient Id', right_on='Patient Id', how='left', suffixes=('','_cbcl'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do logit combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "features = ['Guardian Qnnaire Score', 'Video Score']\n",
    "feature_encoding_map = {feature: 'scalar' for feature in features}\n",
    "### Sample weights not defined for logistic regression in present version of sklearn. \n",
    "### The class_weight='balanced' is the best we can do at the moment.\n",
    "\n",
    "outcome_classes = ['autism', 'not']\n",
    "outcome_class_priors =  [(1.0/2.0), (1.0/2.0)]   \n",
    "\n",
    "logit_mod1_model, features, y_predicted_without_dunno, y_predicted_with_dunno, y_predicted_probs = all_data_model(combined_clinical_df[combined_clinical_df['Video Version']=='module1'], features, feature_encoding_map, target_column='outcome', sample_weight=None,\n",
    "        dunno_range=[0, 0], model_function=linear_model.LogisticRegression, C=100000.0, penalty='l2', class_weight='balanced')\n",
    "\n",
    "logit_mod2_model, features, y_predicted_without_dunno, y_predicted_with_dunno, y_predicted_probs = all_data_model(combined_clinical_df[combined_clinical_df['Video Version']=='module2'], features, feature_encoding_map, target_column='outcome', sample_weight=None,\n",
    "        dunno_range=[0, 0], model_function=linear_model.LogisticRegression, C=100000.0, penalty='l2', class_weight='balanced')\n",
    "\n",
    "def apply_model(model, data_row):\n",
    "    #p= exp(0 + 1*x1 + ... + k*xk)/(1+exp(0 + 1*x1 + ... + k*xk)).\n",
    "    exponent = math.exp(model.intercept_[0] + model.coef_[0][0]*data_row['Guardian Qnnaire Score'] + model.coef_[0][1]*data_row['Video Score'])\n",
    "    probability = float(exponent) / float(1+exponent)\n",
    "    return 1.0 - probability\n",
    "\n",
    "combined_clinical_df['Logit Combinator Score']  = combined_clinical_df.apply(lambda row: apply_model(logit_mod1_model, row) if row['Video Version']=='module1' else apply_model(logit_mod2_model, row), axis=1)\n",
    "\n",
    "print 'mchat and cbcl results: ', combined_clinical_df[['Mchat Final Score', 'CBCL Score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make ROC curves including video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for this_algorithm in ['guardian.qnnaire.3-', 'guardian.qnnaire.4+', 'All ages']:\n",
    "    #if this_algorithm != 'All ages': continue\n",
    "    \n",
    "    if this_algorithm == 'All ages':\n",
    "        print 'len combined clinical df: ', len(combined_clinical_df.index)\n",
    "        df_to_plot = cp.deepcopy(combined_clinical_df)\n",
    "        do_composite_analysis = True\n",
    "    else:\n",
    "        filter_column = algorithms[this_algorithm]['filter_key']\n",
    "        filter_value = algorithms[this_algorithm]['filter_value']\n",
    "        df_to_plot = combined_clinical_df[combined_clinical_df[filter_column]==filter_value]\n",
    "        do_composite_analysis = False\n",
    "        \n",
    "    \n",
    "    this_N = len(df_to_plot.index)\n",
    "    for method in ['normal', 'dunno']:\n",
    "        print 'Do method ', method, ' for algorithm: ', this_algorithm\n",
    "        \n",
    "        questionnaire_info = {'label': 'Questionnaire', 'color': 'blue',\n",
    "                            'linewidth': 5, 'linestyle': 'dashed'}\n",
    "        video_info = {'label': 'Video', 'color': 'red',\n",
    "                     'linewidth': 5, 'linestyle': 'dotted'}\n",
    "        combined_info = {'label': 'Combined questionnaire and video', 'color': 'black', 'color': 'black',\n",
    "                     'linewidth': 5, 'linestyle': 'solid'}\n",
    "        if method == 'dunno':\n",
    "            questionnaire_info['coverage'] = 0.75\n",
    "            video_info['coverage'] = 0.75\n",
    "            combined_info['coverage'] = 0.75\n",
    "        if do_composite_analysis:\n",
    "            questionnaire_classifier_variant_column_name = 'classifier_variant'\n",
    "            video_classifier_variant_column_name = 'Video Version'\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            if method == 'dunno':\n",
    "                \n",
    "                print 'Do composite analysis for questionnaire'\n",
    "                questionnaire_plotting_data = explore_composite_classification_dunno_ranges(df_to_plot, 'outcome',\n",
    "                        'Guardian Qnnaire Score', questionnaire_classifier_variant_column_name, 'autism', 'not',\n",
    "                         specificity_bin_width=desired_specificity_bin_width, desc='questionnaire')\n",
    "                print 'Do composite analysis for video'\n",
    "                sys.stdout.flush()\n",
    "                video_plotting_data = explore_composite_classification_dunno_ranges(df_to_plot, 'outcome',\n",
    "                    'Video Score', video_classifier_variant_column_name, 'autism', 'not',\n",
    "                    specificity_bin_width=desired_specificity_bin_width, desc='video')\n",
    "                print 'Do composite analysis for combination'\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                combined_plotting_data = explore_composite_classification_dunno_ranges(df_to_plot,\n",
    "                        'outcome', 'Logit Combinator Score', questionnaire_classifier_variant_column_name,\n",
    "                        'autism', 'not', specificity_bin_width=desired_specificity_bin_width, desc='combined')\n",
    "            else:   ### method is not dunno\n",
    "                \n",
    "#                 def explore_composite_classification_thresholds(dataframe, target_column_name, response_column_name,\n",
    "#         classifier_variant_column_name, positive_class_name, negative_class_name, specificity_bin_width = 0.025):\n",
    "                questionnaire_plotting_data = explore_composite_classification_thresholds(df_to_plot, 'outcome',\n",
    "                        'Guardian Qnnaire Score', questionnaire_classifier_variant_column_name, 'autism', 'not',\n",
    "                         specificity_bin_width=desired_specificity_bin_width, desc='questionnaire')\n",
    "                pd.set_option('display.max_rows', 50000) \n",
    "                pd.set_option('display.max_columns', 50000)                \n",
    "                print 'Composite questionnaire plotting data: ', questionnaire_plotting_data\n",
    "                print 'Arose from df ', df_to_plot\n",
    "                pd.set_option('display.max_rows', 500)\n",
    "                pd.set_option('display.max_columns', 50)                \n",
    "\n",
    "\n",
    "                \n",
    "                print 'Do composite analysis for video'\n",
    "                sys.stdout.flush()\n",
    "                video_plotting_data = explore_composite_classification_thresholds(df_to_plot, 'outcome',\n",
    "                    'Video Score', video_classifier_variant_column_name, 'autism', 'not',\n",
    "                    specificity_bin_width=desired_specificity_bin_width, desc='video')\n",
    "                print 'Do composite analysis for combination'\n",
    "                sys.stdout.flush()\n",
    "                combined_plotting_data = explore_composite_classification_thresholds(df_to_plot,\n",
    "                        'outcome', 'Logit Combinator Score', questionnaire_classifier_variant_column_name,\n",
    "                        'autism', 'not', specificity_bin_width=desired_specificity_bin_width, desc='combined')\n",
    "        else:    #### Do non-composite analysis (3- or 4+ instead of age combination)\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            if method == 'dunno':\n",
    "                print 'Do non composite analysis for questionnaire'\n",
    "                questionnaire_plotting_data = explore_dunno_ranges(df_to_plot['outcome'],\n",
    "                          df_to_plot['Guardian Qnnaire Score'], 'autism', 'not', desc=this_algorithm+' questionnaire')\n",
    "                print 'Do non composite analysis for video'\n",
    "                sys.stdout.flush()\n",
    "                video_plotting_data = explore_dunno_ranges(df_to_plot['outcome'],\n",
    "                        df_to_plot['Video Score'], 'autism', 'not', desc=this_algorithm +' video')\n",
    "                print 'Do non composite analysis for combination'\n",
    "                sys.stdout.flush()\n",
    "          \n",
    "                combined_plotting_data = explore_dunno_ranges(df_to_plot['outcome'],\n",
    "                        df_to_plot['Logit Combinator Score'], 'autism', 'not', desc=this_algorithm+'combined')\n",
    "            else:   ### method is not dunno\n",
    "#                 def explore_classification_thresholds(target_column, response_column, positive_class_name, negative_class_name):\n",
    "\n",
    "                print 'Do non composite analysis for questionnaire'\n",
    "                questionnaire_plotting_data = explore_classification_thresholds(df_to_plot['outcome'],\n",
    "                          df_to_plot['Guardian Qnnaire Score'], 'autism', 'not', desc=this_algorithm+' questionnaire')\n",
    "                pd.set_option('display.max_rows', 50000) \n",
    "                pd.set_option('display.max_columns', 50000)  \n",
    "                print 'for ', this_algorithm, ', plot questionnaire classifier_data: ', questionnaire_plotting_data\n",
    "                \n",
    "\n",
    "                print 'Do non composite analysis for video'\n",
    "            \n",
    "                sys.stdout.flush()\n",
    "                video_plotting_data = explore_classification_thresholds(df_to_plot['outcome'],\n",
    "                        df_to_plot['Video Score'], 'autism', 'not', desc=this_algorithm+' video')\n",
    "                print 'Do non composite analysis for combination'\n",
    "                sys.stdout.flush()\n",
    "                print 'for ', this_algorithm, ', plot video classifier_data: ', video_plotting_data \n",
    "\n",
    "\n",
    "          \n",
    "                combined_plotting_data = explore_classification_thresholds(df_to_plot['outcome'],\n",
    "                        df_to_plot['Logit Combinator Score'], 'autism', 'not', desc=this_algorithm+' combined')\n",
    "                print 'for ', this_algorithm, ', combined_plotting_data: ', combined_plotting_data\n",
    "\n",
    "        title = ''\n",
    "        plt, fig = plot_classifier_profiles([(mchat_info, mchat_plotting_data), (cbcl_info, cbcl_plotting_data), \n",
    "                (questionnaire_info, questionnaire_plotting_data),\n",
    "                (video_info, video_plotting_data), (combined_info, combined_plotting_data)], \n",
    "                title, ylim=(0.4,1.))\n",
    "        method_file_desc = '' if method == 'normal' else '_dunno'\n",
    "        if purpose_of_run == 'for_plotting':\n",
    "            filename = 'images/combined_with_video_ROC_curves_'+this_algorithm.replace(' ', '_')+method_file_desc+'.png'\n",
    "        else:\n",
    "            filename = 'images/combined_with_video_ROC_curves_'+this_algorithm.replace(' ', '_')+method_file_desc+'_fine_binned.png'\n",
    "\n",
    "        fig.savefig(filename, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
